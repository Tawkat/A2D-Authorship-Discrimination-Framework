{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A2D_Framework_Colab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MpxvQ14HFYM1","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGuiOxBdFh3k","colab_type":"code","colab":{}},"source":["!pip install -q keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"utYxkMR9F0wq","colab_type":"code","colab":{}},"source":["!pip install -q pydrive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIJwvszjF4AZ","colab_type":"code","colab":{}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgGgkW7TGAZx","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, LSTM, GRU, Dense, Embedding,Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import keras.backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mX8p7SpXmZn","colab_type":"code","colab":{}},"source":["\n","from keras import callbacks\n","from keras.models import Sequential\n","from keras.layers import Conv1D,MaxPooling1D,Flatten,Dense,Dropout,Embedding\n","from keras.optimizers import Adam,SGD\n","\n","\n","def create_BiLSTM_model(vocabulary_size,embedding_size,embedding_matrix):\n","    model_glove = Sequential()\n","    model_glove.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], trainable=False))\n","    model_glove.add(Bidirectional(LSTM(100)))\n","    model_glove.add(Dense(1, activation='sigmoid'))\n","    model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model_glove.summary()\n","    return model_glove\n","\n","\n","def callback(checkpoint_path,tf_log_dir_name='./tf-log/',patience_lr=10,):\n","    cb = []\n","    \"\"\"\n","    Tensorboard log callback\n","    \"\"\"\n","    tb = callbacks.TensorBoard(log_dir=tf_log_dir_name, histogram_freq=0)\n","    cb.append(tb)\n","\n","    \"\"\"\n","    Model-Checkpoint\n","    \"\"\"\n","    m = callbacks.ModelCheckpoint(filepath=checkpoint_path,monitor='val_loss',mode='auto',save_best_only=True)\n","    cb.append(m)\n","\n","    \"\"\"\n","    Reduce Learning Rate\n","    \"\"\"\n","    reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n","    cb.append(reduce_lr_loss)\n","\n","    \"\"\"\n","    Early Stopping callback\n","    \"\"\"\n","    # Uncomment for usage\n","    early_stop = callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')\n","    cb.append(early_stop)\n","\n","\n","\n","    return cb\n","\n","######### Show Train Val History Graph ###############\n","def plot_loss_accu(history,lossLoc='Train_Val_Loss',accLoc='Train_Val_acc'):\n","    import matplotlib.pyplot as plt\n","\n","    plt.clf()\n","\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    epochs = range(len(loss))\n","    plt.plot(epochs, loss, 'r')\n","    plt.plot(epochs, val_loss, 'b')\n","    plt.title('Training and validation loss')\n","    plt.legend(['train', 'val'], loc='upper right')\n","    #plt.show()\n","    plt.savefig(lossLoc)\n","\n","    plt.clf()\n","\n","    acc = history.history['acc']\n","    val_acc = history.history['val_acc']\n","    epochs = range(len(acc))\n","    plt.plot(epochs, acc, 'r')\n","    plt.plot(epochs, val_acc, 'b')\n","    plt.title('Training and validation accuracy')\n","    plt.legend(['train', 'val'], loc='lower right')\n","    #plt.show()\n","    plt.savefig(accLoc)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbHKiSTJZjnw","colab_type":"code","colab":{}},"source":["import pickle\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","\n","from sklearn.model_selection import KFold\n","import gc\n","import numpy as np\n","import keras.backend as K\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"etlfyer0TErg","colab_type":"code","colab":{}},"source":["if len(K.tensorflow_backend._get_available_gpus()) > 0:\n","  print(\"GPU Available !!!!!\")\n","  from keras.layers import CuDNNLSTM as LSTM\n","  from keras.layers import CuDNNGRU as GRU"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvhG_8m-ZUHJ","colab_type":"code","colab":{}},"source":["\n","pickle_load = open('Reuter_C50/pickle_train_Reuter_C50_All_train_1_100dim.pickle', 'rb')\n","X_train,y_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_test_Reuter_C50_All_test_1_100dim.pickle', 'rb')\n","X_test,y_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_cleanXy_Reuter_C50_All_train_1.pickle', 'rb')\n","x1_train,y1_train=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_cleanXy_Reuter_C50_All_test_1.pickle', 'rb')\n","x1_test,y1_test=pickle.load(pickle_load)\n","\n","\n","vocabulary_size = embedding_matrix.shape[0]\n","embedding_size=100\n","\n","\n","kfold = KFold(n_splits=5, shuffle=True, random_state=42)##################\n","cvscores=[]\n","classfication_report=[]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYUmZwfg6nKx","colab_type":"code","colab":{}},"source":["# import for showing the confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwqxTYjDynJ3","colab_type":"text"},"source":["# **Model Architecture**"]},{"cell_type":"code","metadata":{"id":"zFR1Nq_uyk40","colab_type":"code","colab":{}},"source":["import keras\n","from keras.layers import Conv1D,MaxPooling1D,Flatten,Dense,Dropout,Embedding\n","\n","max_words_len=300\n","#def create_Att_Model(max_words_len, embedding_size, embedding_matrix):\n","encoder_input=keras.Input(shape=(max_words_len,),)\n","encoder_embedding = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_size,\n","                              weights=[embedding_matrix],\n","                              input_length=max_words_len)\n","\n","\n","encoder_embedding_layer=encoder_embedding(encoder_input)\n","#layer1=Bidirectional(LSTM(NN_dim_1,return_sequences=True))\n","#encoder_output=layer1(encoder_embedding_layer)\n","\n","conv1D_1=Conv1D(512, 4, activation='relu',name='con1')(encoder_embedding_layer)\n","maxPool1D_1=MaxPooling1D(pool_size=2,name='maxpool1')(conv1D_1)\n","\n","conv1D_2=Conv1D(256, 3, activation='relu',name='con2')(maxPool1D_1)\n","maxPool1D_2=MaxPooling1D(pool_size=2,name='maxpool2')(conv1D_2)\n","\n","conv1D_3=Conv1D(128, 2, activation='relu',name='con3')(maxPool1D_2)\n","maxPool1D_3=MaxPooling1D(pool_size=2,name='maxpool3')(conv1D_3)\n","\n","\n","\n","\n","# Apply Bidirectional LSTM over embedded inputs\n","lstm_outs = keras.layers.wrappers.Bidirectional(\n","    keras.layers.LSTM(100, return_sequences=True)\n",")(maxPool1D_3)\n","\n","\n","\n","# Attention Mechanism - Generate attention vectors\n","input_dim = int(lstm_outs.shape[2])\n","permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n","attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n","print(attention_vector.shape)\n","attention_vector = keras.layers.Reshape((attention_vector.shape[1],))(attention_vector)\n","attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n","attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n","\n","# Last layer: fully connected with softmax activation\n","fc = keras.layers.Dense(100, activation='relu')(attention_output)\n","output = keras.layers.Dense(50, activation='softmax')(fc)\n","\n","# Finally building model\n","model = keras.Model(inputs=[encoder_input], outputs=output)\n","model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n","\n","# Print model summary\n","model.summary()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaBf758RzuFX","colab_type":"code","colab":{}},"source":["import os\n","import tensorflow as tf\n","checkpoint_path = \"cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create checkpoint callback\n","cp_callback = callbacks.ModelCheckpoint(filepath=checkpoint_path,monitor='val_loss',mode='auto',save_best_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZd6iiCKv8gL","colab_type":"text"},"source":["# **Reuter C50 All**"]},{"cell_type":"code","metadata":{"id":"NU1b8X_lt3YC","colab_type":"code","colab":{}},"source":["\n","history=model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=128,shuffle=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWOUB17l5uPh","colab_type":"code","colab":{}},"source":["# Re-create the model to get attention vectors as well as label prediction\n","model_with_attentions = keras.Model(inputs=model.input,\n","                                    outputs=[model.output, \n","                                             model.get_layer('attention_vec').output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaC9H_4qbMvo","colab_type":"code","colab":{}},"source":["from sklearn.metrics import precision_recall_fscore_support, classification_report,accuracy_score\n","\n","y_pred, attentions = model_with_attentions.predict(X_test)\n","\n","for y in y_pred:\n","  #print(y)\n","  for i in range(len(y)):\n","    #pass\n","    y[i]=int(y[i]>0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHCfnY7bUJM7","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Reuter_C50_All_1.pickle','wb') as f:\n","    pickle.dump((y_test,y_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQkA_yuuQXqr","colab_type":"code","colab":{}},"source":["from sklearn.metrics import precision_recall_fscore_support, classification_report,accuracy_score\n","\n","print('Accuracy Report:')\n","print(accuracy_score(y_test,y_pred))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJej-gL7AYdl","colab_type":"code","colab":{}},"source":["class_names = [\"0\",\"1\"]\n","\n","# Generate the confusion matrix\n","cnf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# Plot non-normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Spooky Author Classification Confusion matrix')\n","'''plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Spooky Author Classification Confusion matrix, without normalization')'''\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5ompgpvxQds","colab_type":"text"},"source":["# **Freezing Layers**"]},{"cell_type":"code","metadata":{"id":"UwZhpR1srKH_","colab_type":"code","colab":{}},"source":["weightsAndBias = model.get_layer(name='con1').get_weights()\n","weights_before = model.get_layer(name='con1').get_weights()[0]\n","# freeze the weights of this layer\n","model.get_layer(name='con1').trainable = False\n","model.get_layer(name='maxpool1').trainable = False\n","model.get_layer(name='con2').trainable = False\n","model.get_layer(name='maxpool2').trainable = False\n","model.get_layer(name='con3').trainable = False\n","model.get_layer(name='maxpool3').trainable = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKSTbZai2-OY","colab_type":"text"},"source":["# **OUTPUT LAYER CHANGED**"]},{"cell_type":"code","metadata":{"id":"uDDIHbiG28bZ","colab_type":"code","colab":{}},"source":["output = keras.layers.Dense(1, activation='sigmoid')(fc)\n","#output = keras.layers.Dense(3, activation='softmax')(fc)\n","#output = keras.layers.Dense(50, activation='softmax')(fc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7cv68jerRyG","colab_type":"code","colab":{}},"source":["# Finally building model\n","model = keras.Model(inputs=[encoder_input], outputs=output)\n","model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n","#model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n","\n","# Print model summary\n","model.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRPyOSxUyDSH","colab_type":"text"},"source":["# **Quran Bukhari Authorship Classification**"]},{"cell_type":"code","metadata":{"id":"lzNhn-uPmSLB","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Quran_Bukhari_Classification/pickle_train_Quran_Bukhari_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Quran_Bukhari_Classification/pickle_test_Quran_Bukhari_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('pickle_cleanXy_Quran_Bukhari_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-2360,:],y1_random[:-2360,:],x1_random[-2360:,:],y1_random[-2360:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","# evaluate the model\n","print(\"Evaluating Model...\")\n","##########################################\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n","y_pred_class=[]\n","\n","for y in y1_pred:\n","  y[0]=int(y[0]>0.5)\n","\n","from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n","\n","acc=accuracy_score(y1_test,y1_pred)\n","\n","print(\"Eval with Quran Bukhari: \"+str(Fold)+\" \" +str(acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXfIHu_FX5Q8","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Quran_Bukhari_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgaQ7DMbsv_v","colab_type":"code","colab":{}},"source":["class_names = [\"0\",\"1\"]\n","\n","# Generate the confusion matrix\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","\n","# Plot non-normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Quran Bukhari Confusion matrix')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeY0jYf2t2XF","colab_type":"code","colab":{}},"source":["import random\n","import math\n","\n","\n","# VISUALIZATION\n","import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.core.display import display, HTML\n","\n","def rgb_to_hex(rgb):\n","    return '#%02x%02x%02x' % rgb\n","    \n","def attention2color(attention_score,predict,decision='INCORRECT'):\n","    r = 255 - int(attention_score * 255)\n","    if decision=='INCORRECT':\n","      color = rgb_to_hex((255, r, r))\n","    elif predict=='Quran':\n","      color = rgb_to_hex((r, 255, r))\n","    elif predict=='Bukhari':\n","      color = rgb_to_hex((r, r, 255))\n","    #print(color)\n","    return str(color)\n","\n","\n","\n","y1_pred=[]\n","\n","actual1=\"\"\n","predict1=\"\"\n","decision1=\"\"\n","\n","for i in range(len(X1_test)):\n","\n","  tokenized_sample = str(x1_test_random[i]).split(\" \")\n","\n","  # Make predictions\n","  pred, attentions = model_with_attentions.predict(np.reshape(X1_test[i],(1,300)))\n","  y1_pred.append((int)(pred>0.5))\n","\n","  if y1_test[i]==0:\n","    actual1=\"Quran\"\n","  elif y1_test[i]==1:\n","    actual1=\"Bukhari\"\n","  if y1_pred[i]==0:\n","    predict1=\"Quran\"\n","  if y1_pred[i]==1:\n","    predict1=\"Bukhari\"\n","  if y1_test[i]==y1_pred[i]:\n","    decision1=\"Correct\"\n","  else:\n","    decision1=\"INCORRECT\"\n","\n","\n","  token_attention_dic = {}\n","  max_score = 0.0\n","  min_score = 0.0\n","\n","  for token, attention_score in zip(tokenized_sample, attentions[0][:len(tokenized_sample)]):\n"," \n","    token_attention_dic[token] = (math.sqrt(attention_score))\n","\n","      \n","  # Build HTML String to viualize attentions\n","  html_text = \"<hr><p style='font-size: large'><p><b>Actual: \"+actual1+\" Verdict: \"+decision1+\"</b></p><b>Text:  </b>\"\n","  for token, attention in token_attention_dic.items():\n","      html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention, predict1, decision1),\n","                                                                          token)\n","  html_text += \"</p>\"\n","  # Display text enriched with attention scores \n","  display(HTML(html_text))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCPcyxVa6Z-n","colab_type":"text"},"source":["# **Spooky Author All Dataset**"]},{"cell_type":"code","metadata":{"id":"A1VclGIa6YFB","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Spooky Author/pickle_train_Spooky_Author_All_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_test_Spooky_Author_All_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_cleanXy_Spooky_Author_All_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-3916,:],y1_random[:-3916,:],x1_random[-3916:,:],y1_random[-3916:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aV8Fsaj_ZQrP","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Spooky_Author_All_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7U7H0lgQ6z05","colab_type":"code","colab":{}},"source":["import random\n","import math\n","\n","\n","# VISUALIZATION\n","import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.core.display import display, HTML\n","\n","def rgb_to_hex(rgb):\n","    return '#%02x%02x%02x' % rgb\n","    \n","def attention2color(attention_score,predict,decision='INCORRECT'):\n","    r = 255 - int(attention_score * 255)\n","    if decision=='INCORRECT':\n","      color = rgb_to_hex((255, r, r))\n","    elif predict=='EAP':\n","      color = rgb_to_hex((r, 255, r))\n","    elif predict=='MWS':\n","      color = rgb_to_hex((r, r, 255))\n","    #print(color)\n","    return str(color)\n","\n","\n","\n","y1_pred=[]\n","\n","actual1=\"\"\n","predict1=\"\"\n","decision1=\"\"\n","\n","for i in range(len(X1_test)):\n","  \n","  tokenized_sample = str(x1_test_random[i]).split(\" \")\n","\n","  # Make predictions\n","  pred, attentions = model_with_attentions.predict(np.reshape(X1_test[i],(1,300)))\n","  \n","  y1_pred.append((int)(pred>0.5))\n","\n","  if y1_test[i]==0:\n","    actual1=\"EAP\"\n","  elif y1_test[i]==1:\n","    actual1=\"MWS\"\n","  if y1_pred[i]==0:\n","    predict1=\"EAP\"\n","  if y1_pred[i]==1:\n","    predict1=\"MWS\"\n","  if y1_test[i]==y1_pred[i]:\n","    decision1=\"Correct\"\n","  else:\n","    decision1=\"INCORRECT\"\n","\n","\n","\n","  token_attention_dic = {}\n","  max_score = 0.0\n","  min_score = 0.0\n","  \n","  for token, attention_score in zip(tokenized_sample, attentions[0][:len(tokenized_sample)]):\n","    \n","    token_attention_dic[token] = (math.sqrt(attention_score))\n","\n","      \n","  # Build HTML String to viualize attentions\n","  html_text = \"<hr><p style='font-size: large'><p><b>Actual: \"+actual1+\" Predict: \"+predict1+\" Verdict: \"+decision1+\"</b></p><b>Text:  </b>\"\n","  for token, attention in token_attention_dic.items():\n","      \n","      html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention, predict1, decision1),\n","                                                                          token)\n","  html_text += \"</p>\"\n","  # Display text enriched with attention scores \n","  display(HTML(html_text))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTefbix_hxzJ","colab_type":"text"},"source":["# **Spooky Author Binary**"]},{"cell_type":"code","metadata":{"id":"-TBfwAxph2uQ","colab_type":"code","colab":{}},"source":["\n","print(\"Fitting Model...\")\n","\n","pickle_load = open('Spooky Author/pickle_train_Spooky_Author_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_test_Spooky_Author_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_cleanXy_Spooky_Author_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-2789,:],y1_random[:-2789,:],x1_random[-2789:,:],y1_random[-2789:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","print(\"Evaluating Model...\")\n","##########################################\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n","y_pred_class=[]\n","\n","for y in y1_pred:\n","  y[0]=int(y[0]>0.5)\n","\n","acc=accuracy_score(y1_test,y1_pred)\n","\n","print(\"Eval with Spooky Author: \"+str(Fold)+\" \" +str(acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrP29H83h7rc","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Spooky_Author_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKl8JJs8r6yt","colab_type":"code","colab":{}},"source":["class_names = [\"0\",\"1\"]\n","\n","# Generate the confusion matrix\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","\n","# Plot non-normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Spooky Author Confusion matrix')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4dkwEM_rjhb","colab_type":"code","colab":{}},"source":["import random\n","import math\n","\n","\n","# VISUALIZATION\n","import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.core.display import display, HTML\n","\n","def rgb_to_hex(rgb):\n","    return '#%02x%02x%02x' % rgb\n","    \n","def attention2color(attention_score,predict,decision='INCORRECT'):\n","    r = 255 - int(attention_score * 255)\n","    if decision=='INCORRECT':\n","      color = rgb_to_hex((255, r, r))\n","    elif predict=='EAP':\n","      color = rgb_to_hex((r, 255, r))\n","    elif predict=='MWS':\n","      color = rgb_to_hex((r, r, 255))\n","    #print(color)\n","    return str(color)\n","\n","\n","y1_pred=[]\n","\n","actual1=\"\"\n","predict1=\"\"\n","decision1=\"\"\n","\n","for i in range(len(X1_test)):\n","\n","  tokenized_sample = str(x1_test_random[i]).split(\" \")\n","\n","\n","  # Make predictions\n","  pred, attentions = model_with_attentions.predict(np.reshape(X1_test[i],(1,300)))\n","  y1_pred.append((int)(pred>0.5))\n","\n","  if y1_test[i]==0:\n","    actual1=\"EAP\"\n","  elif y1_test[i]==1:\n","    actual1=\"MWS\"\n","  if y1_pred[i]==0:\n","    predict1=\"EAP\"\n","  if y1_pred[i]==1:\n","    predict1=\"MWS\"\n","  if y1_test[i]==y1_pred[i]:\n","    decision1=\"Correct\"\n","  else:\n","    decision1=\"INCORRECT\"\n","\n","\n","  token_attention_dic = {}\n","  max_score = 0.0\n","  min_score = 0.0\n","  for token, attention_score in zip(tokenized_sample, attentions[0][:len(tokenized_sample)]):\n","    token_attention_dic[token] = (math.sqrt(attention_score))\n","\n","      \n","  # Build HTML String to viualize attentions\n","  html_text = \"<hr><p style='font-size: large'><p><b>Actual: \"+actual1+\" Predict: \"+predict1+\" Verdict: \"+decision1+\"</b></p><b>Text:  </b>\"\n","  for token, attention in token_attention_dic.items():\n","      \n","      html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention, predict1, decision1),\n","                                                                          token)\n","  html_text += \"</p>\"\n","  # Display text enriched with attention scores \n","  display(HTML(html_text))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpAMt2J7eGVN","colab_type":"text"},"source":["# **Washington Irving**"]},{"cell_type":"code","metadata":{"id":"HsZ36zKEd-l9","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Rip Sketch/pickle_train_New_Sketch_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Rip Sketch/pickle_test_New_Sketch_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Rip Sketch/pickle_cleanXy_New_Sketch_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-1460,:],y1_random[:-1460,:],x1_random[-1460:,:],y1_random[-1460:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","# evaluate the model\n","print(\"Evaluating Model...\")\n","##########################################\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n","y_pred_class=[]\n","\n","for y in y1_pred:\n","  y[0]=int(y[0]>0.5)\n","\n","acc=accuracy_score(y1_test,y1_pred)\n","\n","print(\"Eval with Spooky Author: \"+str(Fold)+\" \" +str(acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hByLvGAeR5T","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_New_Sketch_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TG9NYLqgDJL","colab_type":"code","colab":{}},"source":["class_names = [\"Geoffrey Crayon\", \"Diedrich Knickerbocker\"]\n","\n","# Generate the confusion matrix\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","\n","# Plot non-normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Washington Irving Confusion matrix')\n","\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OB46IxhsgawV","colab_type":"code","colab":{}},"source":["import random\n","import math\n","\n","# VISUALIZATION\n","import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.core.display import display, HTML\n","\n","def rgb_to_hex(rgb):\n","    return '#%02x%02x%02x' % rgb\n","    \n","def attention2color(attention_score,predict,decision='INCORRECT'):\n","    r = 255 - int(attention_score * 255)\n","    if decision=='INCORRECT':\n","      color = rgb_to_hex((255, r, r))\n","    elif predict=='Geoffrey Crayon':\n","      color = rgb_to_hex((r, 255, r))\n","    elif predict=='Diedrich Knickerbocker':\n","      color = rgb_to_hex((r, r, 255))\n","    #print(color)\n","    return str(color)\n","\n","\n","\n","\n","y1_pred=[]\n","\n","actual1=\"\"\n","predict1=\"\"\n","decision1=\"\"\n","\n","for i in range(len(X1_test)):\n","\n","  tokenized_sample = str(x1_test_random[i]).split(\" \")\n","\n","  # Make predictions\n","  pred, attentions = model_with_attentions.predict(np.reshape(X1_test[i],(1,300)))\n","\n","  y1_pred.append((int)(pred>0.5))\n","\n","  if y1_test[i]==0:\n","    actual1=\"Geoffrey Crayon\"\n","  elif y1_test[i]==1:\n","    actual1=\"Diedrich Knickerbocker\"\n","  if y1_pred[i]==0:\n","    predict1=\"Geoffrey Crayon\"\n","  if y1_pred[i]==1:\n","    predict1=\"Diedrich Knickerbocker\"\n","  if y1_test[i]==y1_pred[i]:\n","    decision1=\"Correct\"\n","  else:\n","    decision1=\"INCORRECT\"\n","\n","\n","  token_attention_dic = {}\n","  max_score = 0.0\n","  min_score = 0.0\n","\n","  for token, attention_score in zip(tokenized_sample, attentions[0][:len(tokenized_sample)]):\n","\n","    token_attention_dic[token] = (math.sqrt(attention_score))\n","\n","      \n","  # Build HTML String to viualize attentions\n","  html_text = \"<hr><p style='font-size: large'><p><b>Actual: \"+actual1+\" Predict: \"+predict1+\" Verdict: \"+decision1+\"</b></p><b>Text:  </b>\"\n","  for token, attention in token_attention_dic.items():\n","\n","      html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention, predict1, decision1),\n","                                                                          token)\n","  html_text += \"</p>\"\n","  # Display text enriched with attention scores \n","  display(HTML(html_text))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctdybD6F5IE9","colab_type":"code","colab":{}},"source":["import random\n","import math\n","\n","\n","y1_pred=[]\n","\n","att_Geoffrey_list=[]\n","att_Knickerbocker_list=[]\n","\n","actual1=\"\"\n","predict1=\"\"\n","decision1=\"\"\n","\n","for i in range(len(X1_test)):\n","\n","  pred, attentions = model_with_attentions.predict(np.reshape(X1_test[i],(1,300)))\n","  y1_pred.append((int)(pred>0.5))\n","\n","  if y1_test[i]==0:\n","    actual1=\"Geoffrey\"\n","  elif y1_test[i]==1:\n","    actual1=\"Knickerbocker\"\n","  if y1_pred[i]==0:\n","    predict1=\"Geoffrey\"\n","  if y1_pred[i]==1:\n","    predict1=\"Knickerbocker\"\n","  if y1_test[i]==y1_pred[i]:\n","    decision1=\"Correct\"\n","  else:\n","    decision1=\"INCORRECT\"\n","  if y1_test[i]==0:\n","      att_Geoffrey_list.append(attentions)\n","  elif y1_test[i]==1:\n","      att_Knickerbocker_list.append(attentions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVVhn8q559MU","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","att_Geoffrey_arr=np.sum(att_Geoffrey_list, axis=0,keepdims=False)\n","print(att_Geoffrey_arr.shape)\n","att_Geoffrey_arr=att_Geoffrey_arr.reshape((att_Geoffrey_arr.shape[1],))\n","att_Geoffrey_arr=att_Geoffrey_arr/(np.max(att_Geoffrey_arr))\n","\n","plt.plot(att_Geoffrey_arr,c='y')\n","#plt.show()\n","\n","att_Knickerbocker_arr=np.sum(att_Knickerbocker_list, axis=0,keepdims=False)\n","#print(att_Knickerbocker_arr)\n","att_Knickerbocker_arr=att_Knickerbocker_arr.reshape((att_Knickerbocker_arr.shape[1],))\n","\n","att_Knickerbocker_arr=att_Knickerbocker_arr/(np.max(att_Knickerbocker_arr))\n","plt.plot(att_Knickerbocker_arr)\n","\n","\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5q8ipAx6k4M","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Attention_Score/pickle_Att_New_Sketch_1.pickle','wb') as f:\n","    pickle.dump((att_Geoffrey_arr,att_Knickerbocker_arr),f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NlfWiWmBGkF2","colab_type":"text"},"source":["# **Random Reuter C50 All**"]},{"cell_type":"code","metadata":{"id":"QW0a4HNhGg0p","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Reuter_C50/pickle_train_Reuter_C50_All_Random_train_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_test_Reuter_C50_All_Random_test_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_cleanXy_Reuter_C50_All_Random_train_1.pickle', 'rb')\n","x1_train_random,y1_train_random,=pickle.load(pickle_load)\n","\n","pickle_load = open('Reuter_C50/pickle_cleanXy_Reuter_C50_All_Random_test_1.pickle', 'rb')\n","x1_test_random,y1_test_random=pickle.load(pickle_load)\n","\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=20, batch_size=128,shuffle=True)\n","\n","\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0R4-_y3ZhJm","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Random_Reuter_C50_All_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8N0MJWRi35Rh","colab_type":"text"},"source":["# **Random Spooky Author Binary**"]},{"cell_type":"code","metadata":{"id":"tlD5aqmb3xY0","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Spooky Author/pickle_train_Spooky_Author_Random_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('/Spooky Author/pickle_test_Spooky_Author_Random_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_cleanXy_Spooky_Author_Random_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-2789,:],y1_random[:-2789,:],x1_random[-2789:,:],y1_random[-2789:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","# evaluate the model\n","print(\"Evaluating Model...\")\n","##########################################\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n","y_pred_class=[]\n","\n","for y in y1_pred:\n","  y[0]=int(y[0]>0.5)\n","\n","acc=accuracy_score(y1_test,y1_pred)\n","\n","print(\"Eval with Spooky Author: \"+str(Fold)+\" \" +str(acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsPx0OKc4kXK","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Spooky_Author_Random_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAHk367F6cD-","colab_type":"code","colab":{}},"source":["class_names = [\"0\",\"1\"]\n","\n","# Generate the confusion matrix\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","\n","# Plot non-normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Spooky Author Random Confusion matrix')\n","\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCHEGzk76_pg","colab_type":"text"},"source":["# **Random Spooky Author All**"]},{"cell_type":"code","metadata":{"id":"ujp-_pan62jh","colab_type":"code","colab":{}},"source":["print(\"Fitting Model...\")\n","\n","pickle_load = open('Spooky Author/pickle_train_Spooky_Author_All_Random_1_100dim.pickle', 'rb')\n","X1_train,y1_train,embedding_matrix=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_test_Spooky_Author_All_Random_1_100dim.pickle', 'rb')\n","X1_test,y1_test,_=pickle.load(pickle_load)\n","\n","pickle_load = open('Spooky Author/pickle_cleanXy_Spooky_Author_All_Random_1.pickle', 'rb')\n","x1_random,y1_random=pickle.load(pickle_load)\n","\n","x1_train_random,y1_train_random,x1_test_random,y1_test_random=x1_random[:-3916,:],y1_random[:-3916,:],x1_random[-3916:,:],y1_random[-3916:,:]\n","\n","history=model.fit(X1_train, y1_train, validation_split=0.1, epochs=15, batch_size=128,shuffle=True)\n","\n","\n","\n","y1_pred, attentions = model_with_attentions.predict(X1_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxFIO2x97KKF","colab_type":"code","colab":{}},"source":["import pickle\n","with open('Predictions/pickle_Pred_Spooky_Author_All_Random_1.pickle','wb') as f:\n","    pickle.dump((y1_test,y1_pred),f)"],"execution_count":0,"outputs":[]}]}